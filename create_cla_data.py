import pandas as pd
import os
import re
from rdkit import Chem
from rdkit import rdBase
from functools import reduce
from sklearn.model_selection import train_test_split
import numpy as np

# ç¦ç”¨RDKitçš„è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼Œä»¥ä¾¿æˆ‘ä»¬è‡ªå·±æ•è·å’Œå¤„ç†
rdBase.DisableLog('rdApp.error')

# --- æ ¸å¿ƒè¾…åŠ©å‡½æ•° ---

def diagnose_and_fix_smiles(smiles: str) -> dict:
    """
    è¯Šæ–­å¹¶å°è¯•ä¿®å¤å•ä¸ªSMILESå­—ç¬¦ä¸²ã€‚
    """
    if not isinstance(smiles, str) or not smiles:
        return {'status': 'unfixable', 'smiles': None, 'error': 'Input is not a valid string.'}

    # 1. å°è¯•åŸå§‹SMILES
    try:
        mol = Chem.MolFromSmiles(smiles, sanitize=True)
        if mol is not None:
            return {'status': 'valid', 'smiles': smiles, 'error': None}
    except Exception as e:
        pass # ç»§ç»­å°è¯•ä¿®å¤

    # 2. åº”ç”¨ä¿®å¤ç­–ç•¥
    smiles_fixed = smiles
    
    # å¤„ç†å¸¸è§çš„å…ƒç´ ç¬¦å·é”™è¯¯
    element_typo_replacements = {'IN': '[In]'}
    for typo, correction in element_typo_replacements.items():
        smiles_fixed = re.sub(r'(^|[^a-zA-Z])' + re.escape(typo) + r'($|[^a-zA-Z])', r'\1' + correction + r'\2', smiles_fixed)

    # è½¬æ¢ [N+H] é£æ ¼ä¸º [NH+] é£æ ¼
    charge_hydrogen_format_replacements = {
        '[N+H]': '[NH+]', '[N+H2]': '[NH2+]', '[N+H3]': '[NH3+]', '[n+H]': '[nH+]', '[O+H]': '[OH+]'
    }
    for old, new in charge_hydrogen_format_replacements.items():
        smiles_fixed = smiles_fixed.replace(old, new)
    
    # ç§»é™¤å…¶ä»–å¸¦ç”µåŸå­ä¸­ä¸å¿…è¦çš„æ˜¾å¼æ°¢åŸå­
    smiles_fixed = re.sub(r'\[([A-Za-z@\*]+[+\-]\d+)H\d*\]', r'[\1]', smiles_fixed)

    # 3. å°è¯•è§£æä¿®å¤åçš„SMILES
    try:
        mol = Chem.MolFromSmiles(smiles_fixed, sanitize=True)
        if mol is not None:
            if smiles_fixed != smiles:
                return {'status': 'fixed', 'smiles': smiles_fixed, 'error': None}
            else:
                return {'status': 'valid', 'smiles': smiles, 'error': None}
    except Exception as e:
        return {'status': 'unfixable', 'smiles': None, 'error': str(e)}

    return {'status': 'unfixable', 'smiles': None, 'error': 'Initial parsing failed and no fixes were applicable.'}


def create_stratification_label(df, task_columns):
    """
    ä¸ºåˆ†å±‚é‡‡æ ·åˆ›å»ºåˆ†å±‚æ ‡ç­¾
    å¯¹äºå¤šä»»åŠ¡åˆ†ç±»ï¼Œåˆ›å»ºä¸€ä¸ªç»„åˆæ ‡ç­¾ç”¨äºåˆ†å±‚
    """
    # æ–¹æ³•1ï¼šä½¿ç”¨ä¸»è¦ä»»åŠ¡ï¼ˆæœ‰æ•°æ®æœ€å¤šçš„ä»»åŠ¡ï¼‰è¿›è¡Œåˆ†å±‚
    task_counts = {}
    for task in task_columns:
        task_counts[task] = df[task].notna().sum()
    
    # é€‰æ‹©æ•°æ®æœ€å¤šçš„ä»»åŠ¡ä½œä¸ºä¸»åˆ†å±‚ä»»åŠ¡
    main_task = max(task_counts, key=task_counts.get)
    print(f"  ä½¿ç”¨ {main_task} ä½œä¸ºä¸»åˆ†å±‚ä»»åŠ¡ ({task_counts[main_task]} ä¸ªæœ‰æ•ˆæ ·æœ¬)")
    
    # åˆ›å»ºåˆ†å±‚æ ‡ç­¾
    stratify_labels = []
    for idx, row in df.iterrows():
        if pd.notna(row[main_task]):
            # ä½¿ç”¨ä¸»ä»»åŠ¡çš„æ ‡ç­¾
            stratify_labels.append(f"{main_task}_{int(row[main_task])}")
        else:
            # å¦‚æœä¸»ä»»åŠ¡æ²¡æœ‰æ ‡ç­¾ï¼ŒæŸ¥æ‰¾å…¶ä»–æœ‰æ ‡ç­¾çš„ä»»åŠ¡
            found_label = False
            for task in task_columns:
                if pd.notna(row[task]):
                    stratify_labels.append(f"{task}_{int(row[task])}")
                    found_label = True
                    break
            if not found_label:
                stratify_labels.append("no_label")
    
    return stratify_labels


def process_and_split_datasets(input_dir, output_dir, files_to_skip=None, split_ratios=(0.8, 0.1, 0.1), random_state=42):
    """
    åŠ è½½ã€ä¿®å¤ã€åˆå¹¶å¹¶åˆ†å‰²CSVæ–‡ä»¶ï¼Œè¾“å‡ºtrain.csv, val.csv, test.csv
    ä¸“é—¨é’ˆå¯¹åˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨åˆ†å±‚é‡‡æ ·
    """
    if files_to_skip is None:
        files_to_skip = []
    
    skip_list_lower = [item.lower().strip() for item in files_to_skip]
    all_processed_dfs = []

    print(f"å¼€å§‹å¤„ç†ç›®å½• '{input_dir}' ...")
    print(f"åˆ†å‰²æ¯”ä¾‹: è®­ç»ƒé›† {split_ratios[0]:.1%}, éªŒè¯é›† {split_ratios[1]:.1%}, æµ‹è¯•é›† {split_ratios[2]:.1%}")
    print(f"ä½¿ç”¨åˆ†å±‚é‡‡æ ·è¿›è¡Œåˆ†ç±»ä»»åŠ¡åˆ†å‰²")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # å¤„ç†æ‰€æœ‰ä»»åŠ¡æ–‡ä»¶
    for filename in os.listdir(input_dir):
        if not filename.endswith('.csv'): 
            continue
        
        task_name_original = filename.split('.')[0]
        task_name_normalized = task_name_original.strip().lower()

        if task_name_normalized in skip_list_lower: 
            print(f"è·³è¿‡æ–‡ä»¶: {filename}")
            continue
            
        print(f"å¤„ç†: {filename}")
        file_path = os.path.join(input_dir, filename)

        try:
            df = pd.read_csv(file_path)
            if 'SMILES' in df.columns: 
                df.rename(columns={'SMILES': 'smiles'}, inplace=True)
            if 'smiles' not in df.columns or 'Label' not in df.columns: 
                continue

            # è¯Šæ–­å’Œä¿®å¤SMILES
            diagnostics = df['smiles'].apply(diagnose_and_fix_smiles)
            df['smiles_fixed'] = diagnostics.apply(lambda x: x['smiles'])
            df['status'] = diagnostics.apply(lambda x: x['status'])

            # æ¸…æ´—æ•°æ®ï¼šåªä¿ç•™æœ‰æ•ˆå’Œå·²ä¿®å¤çš„SMILES
            cleaned_df = df[df['status'].isin(['valid', 'fixed'])].copy()
            if cleaned_df.empty: 
                continue

            # é‡å‘½åLabelåˆ—å¹¶åˆ›å»ºæœ€ç»ˆæ•°æ®æ¡†ï¼ˆä¸åŒ…å«Inchikeyï¼‰
            cleaned_df.rename(columns={'Label': task_name_original}, inplace=True)
            final_df = cleaned_df[['smiles_fixed', task_name_original]].copy()
            final_df.rename(columns={'smiles_fixed': 'smiles'}, inplace=True)
            
            if not final_df.empty:
                all_processed_dfs.append(final_df)
                print(f"  -> {len(final_df)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
                
                # æ˜¾ç¤ºåˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ
                label_counts = final_df[task_name_original].value_counts().sort_index()
                print(f"  -> æ ‡ç­¾åˆ†å¸ƒ: {dict(label_counts)}")

        except Exception as e:
            print(f"  -> å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")

    # åˆå¹¶æ‰€æœ‰ä»»åŠ¡æ•°æ®
    if not all_processed_dfs:
        print("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä»¥åˆå¹¶ï¼")
        return

    print(f"\nåˆå¹¶ {len(all_processed_dfs)} ä¸ªä»»åŠ¡çš„æ•°æ®...")
    
    # ä½¿ç”¨smilesä½œä¸ºå”¯ä¸€é”®è¿›è¡Œåˆå¹¶ï¼ˆå»æ‰Inchikeyï¼‰
    complete_dataset = all_processed_dfs[0]
    for df in all_processed_dfs[1:]:
        complete_dataset = pd.merge(complete_dataset, df, on='smiles', how='outer')
    
    print(f"åˆå¹¶åæ•°æ®é›†: {len(complete_dataset)} ä¸ªå”¯ä¸€åˆ†å­")
    
    # è·å–ä»»åŠ¡åˆ—ï¼ˆé™¤äº†smilesåˆ—ï¼‰
    task_columns = [col for col in complete_dataset.columns if col != 'smiles']
    print(f"ä»»åŠ¡åˆ—: {task_columns}")
    
    # æ˜¾ç¤ºå„ä»»åŠ¡çš„æ ‡ç­¾åˆ†å¸ƒ
    print(f"\nå„ä»»åŠ¡æ ‡ç­¾åˆ†å¸ƒ:")
    for task in task_columns:
        valid_data = complete_dataset[task].dropna()
        if len(valid_data) > 0:
            label_counts = valid_data.value_counts().sort_index()
            print(f"  {task}: {dict(label_counts)} (æ€»è®¡ {len(valid_data)} æ ·æœ¬)")
        else:
            print(f"  {task}: æ— æœ‰æ•ˆæ•°æ®")
    
    # åˆ†å‰²æ•°æ®é›†ï¼ˆä½¿ç”¨åˆ†å±‚é‡‡æ ·ï¼‰
    total_samples = len(complete_dataset)
    test_size = split_ratios[2]
    val_size = split_ratios[1]
    
    print(f"\nå¼€å§‹åˆ†å±‚åˆ†å‰²æ•°æ®é›†...")
    
    if total_samples < 10:
        print("è­¦å‘Š: æ•°æ®é‡å¤ªå°‘ï¼Œä¸è¿›è¡Œåˆ†å‰²")
        train_data = complete_dataset.copy()
        val_data = pd.DataFrame(columns=complete_dataset.columns)
        test_data = pd.DataFrame(columns=complete_dataset.columns)
    else:
        try:
            # åˆ›å»ºåˆ†å±‚æ ‡ç­¾
            stratify_labels = create_stratification_label(complete_dataset, task_columns)
            
            # æ£€æŸ¥æ¯ä¸ªåˆ†å±‚æ ‡ç­¾çš„æ ·æœ¬æ•°é‡
            from collections import Counter
            label_counts = Counter(stratify_labels)
            print(f"  åˆ†å±‚æ ‡ç­¾åˆ†å¸ƒ: {dict(label_counts)}")
            
            # è¿‡æ»¤æ‰æ ·æœ¬æ•°é‡è¿‡å°‘çš„æ ‡ç­¾ï¼ˆè‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬æ‰èƒ½åˆ†å±‚ï¼‰
            valid_indices = []
            valid_stratify_labels = []
            
            for i, label in enumerate(stratify_labels):
                if label_counts[label] >= 2:  # è‡³å°‘2ä¸ªæ ·æœ¬æ‰èƒ½åˆ†å±‚
                    valid_indices.append(i)
                    valid_stratify_labels.append(label)
            
            if len(valid_indices) < len(complete_dataset) * 0.5:
                print("  è­¦å‘Š: å¯ç”¨äºåˆ†å±‚çš„æ ·æœ¬å¤ªå°‘ï¼Œæ”¹ç”¨éšæœºåˆ†å‰²")
                # ä½¿ç”¨éšæœºåˆ†å‰²
                train_val_data, test_data = train_test_split(
                    complete_dataset, 
                    test_size=test_size, 
                    random_state=random_state
                )
                
                if len(train_val_data) < 5:
                    train_data = train_val_data.copy()
                    val_data = pd.DataFrame(columns=complete_dataset.columns)
                else:
                    val_size_adjusted = val_size / (1 - test_size)
                    train_data, val_data = train_test_split(
                        train_val_data,
                        test_size=val_size_adjusted,
                        random_state=random_state
                    )
            else:
                # ä½¿ç”¨åˆ†å±‚åˆ†å‰²
                valid_data = complete_dataset.iloc[valid_indices]
                remaining_data = complete_dataset.drop(complete_dataset.index[valid_indices])
                
                print(f"  ä½¿ç”¨åˆ†å±‚åˆ†å‰²: {len(valid_data)} æ ·æœ¬")
                print(f"  æ— æ³•åˆ†å±‚çš„æ ·æœ¬: {len(remaining_data)} æ ·æœ¬")
                
                # å¯¹å¯åˆ†å±‚çš„æ•°æ®è¿›è¡Œåˆ†å±‚åˆ†å‰²
                train_val_valid, test_valid = train_test_split(
                    valid_data,
                    test_size=test_size,
                    stratify=valid_stratify_labels,
                    random_state=random_state
                )
                
                # å¯¹å‰©ä½™æ•°æ®è¿›è¡Œéšæœºåˆ†å‰²
                if len(remaining_data) > 0:
                    train_val_remain, test_remain = train_test_split(
                        remaining_data,
                        test_size=test_size,
                        random_state=random_state
                    )
                    
                    # åˆå¹¶åˆ†å±‚å’Œéšæœºåˆ†å‰²çš„ç»“æœ
                    train_val_data = pd.concat([train_val_valid, train_val_remain], ignore_index=True)
                    test_data = pd.concat([test_valid, test_remain], ignore_index=True)
                else:
                    train_val_data = train_val_valid
                    test_data = test_valid
                
                # ç»§ç»­åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
                if len(train_val_data) < 5:
                    train_data = train_val_data.copy()
                    val_data = pd.DataFrame(columns=complete_dataset.columns)
                else:
                    val_size_adjusted = val_size / (1 - test_size)
                    
                    # å°è¯•å¯¹è®­ç»ƒ+éªŒè¯é›†è¿›è¡Œåˆ†å±‚åˆ†å‰²
                    try:
                        train_val_stratify = create_stratification_label(train_val_data, task_columns)
                        train_val_label_counts = Counter(train_val_stratify)
                        
                        # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ†å±‚
                        can_stratify = all(count >= 2 for count in train_val_label_counts.values())
                        
                        if can_stratify and len(set(train_val_stratify)) > 1:
                            train_data, val_data = train_test_split(
                                train_val_data,
                                test_size=val_size_adjusted,
                                stratify=train_val_stratify,
                                random_state=random_state
                            )
                            print("  è®­ç»ƒ/éªŒè¯é›†ä¹Ÿä½¿ç”¨äº†åˆ†å±‚åˆ†å‰²")
                        else:
                            train_data, val_data = train_test_split(
                                train_val_data,
                                test_size=val_size_adjusted,
                                random_state=random_state
                            )
                            print("  è®­ç»ƒ/éªŒè¯é›†ä½¿ç”¨éšæœºåˆ†å‰²")
                    except:
                        train_data, val_data = train_test_split(
                            train_val_data,
                            test_size=val_size_adjusted,
                            random_state=random_state
                        )
                        print("  è®­ç»ƒ/éªŒè¯é›†åˆ†å±‚å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆ†å‰²")
            
        except Exception as e:
            print(f"  åˆ†å±‚åˆ†å‰²å¤±è´¥: {e}")
            print("  æ”¹ç”¨éšæœºåˆ†å‰²...")
            
            # é™çº§åˆ°éšæœºåˆ†å‰²
            train_val_data, test_data = train_test_split(
                complete_dataset, 
                test_size=test_size, 
                random_state=random_state
            )
            
            if len(train_val_data) < 5:
                train_data = train_val_data.copy()
                val_data = pd.DataFrame(columns=complete_dataset.columns)
            else:
                val_size_adjusted = val_size / (1 - test_size)
                train_data, val_data = train_test_split(
                    train_val_data,
                    test_size=val_size_adjusted,
                    random_state=random_state
                )

    print(f"\nåˆ†å‰²ç»“æœ:")
    print(f"  è®­ç»ƒé›†: {len(train_data):,} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_data):,} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_data):,} æ ·æœ¬")

    # æ˜¾ç¤ºå„æ•°æ®é›†çš„æ ‡ç­¾åˆ†å¸ƒ
    for dataset_name, dataset in [("è®­ç»ƒé›†", train_data), ("éªŒè¯é›†", val_data), ("æµ‹è¯•é›†", test_data)]:
        if not dataset.empty:
            print(f"\n{dataset_name}æ ‡ç­¾åˆ†å¸ƒ:")
            for task in task_columns:
                valid_data = dataset[task].dropna()
                if len(valid_data) > 0:
                    label_counts = valid_data.value_counts().sort_index()
                    print(f"  {task}: {dict(label_counts)}")

    # ä¿å­˜åˆ†å‰²åçš„æ•°æ®é›†
    if not train_data.empty:
        train_path = os.path.join(output_dir, 'train.csv')
        train_data.to_csv(train_path, index=False)
        print(f"\nè®­ç»ƒé›†å·²ä¿å­˜è‡³: {train_path}")

    if not val_data.empty:
        val_path = os.path.join(output_dir, 'val.csv')
        val_data.to_csv(val_path, index=False)
        print(f"éªŒè¯é›†å·²ä¿å­˜è‡³: {val_path}")

    if not test_data.empty:
        test_path = os.path.join(output_dir, 'test.csv')
        test_data.to_csv(test_path, index=False)
        print(f"æµ‹è¯•é›†å·²ä¿å­˜è‡³: {test_path}")

    print(f"\nå¤„ç†å®Œæˆï¼è¾“å‡ºæ–‡ä»¶:")
    print(f"  ğŸ“ {output_dir}/train.csv")
    print(f"  ğŸ“ {output_dir}/val.csv")
    print(f"  ğŸ“ {output_dir}/test.csv")


# --- æ‰§è¡Œ ---
if __name__ == '__main__':
    # æµ‹è¯•è¯Šæ–­åŠŸèƒ½
    print("--- æµ‹è¯•è¯Šæ–­åŠŸèƒ½ ---")
    test_smiles = 'BrN([C-2]C)([C-2]C)([C-2]C)[C-2]C'
    result = diagnose_and_fix_smiles(test_smiles)
    print(f"åŸå§‹SMILES: {test_smiles}")
    print(f"è¯Šæ–­ç»“æœ: {result}")
    print("-" * 50)
    
    # è¿è¡Œæ•°æ®åˆ†å‰²å’Œåˆå¹¶æµç¨‹
    INPUT_CLA_DIR = 'ademt_data/wash_cla'  # åˆ†ç±»æ•°æ®ç›®å½•
    OUTPUT_DIR = 'ademt_data/cla_3'
    SKIP_FILES = []  # å¯ä»¥æŒ‡å®šè¦è·³è¿‡çš„æ–‡ä»¶
    
    # è®¾ç½®åˆ†å‰²å‚æ•°
    SPLIT_RATIOS = (0.8, 0.1, 0.1)  # è®­ç»ƒé›†, éªŒè¯é›†, æµ‹è¯•é›†
    RANDOM_STATE = 587 # éšæœºç§å­

    process_and_split_datasets(
        input_dir=INPUT_CLA_DIR, 
        output_dir=OUTPUT_DIR, 
        files_to_skip=SKIP_FILES,
        split_ratios=SPLIT_RATIOS,
        random_state=RANDOM_STATE
    )     # è®­ç»ƒé›†, éªŒè¯é›†, æµ‹è¯•é›†

    process_and_split_datasets(
        input_dir=INPUT_CLA_DIR, 
        output_dir=OUTPUT_DIR, 
        files_to_skip=SKIP_FILES,
        split_ratios=SPLIT_RATIOS,
        random_state=RANDOM_STATE
    )