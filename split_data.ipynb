{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8ab15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c636a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in os.listdir('ademt_data'):\n",
    "#     if file.endswith('.csv'):\n",
    "#         df = pd.read_csv(f'ademt_data/{file}')\n",
    "#         print(df.head())\n",
    "        \n",
    "#         if len(set(list(df['Label']))) == 2:\n",
    "#             os.system(f'cp ademt_data/{file} ./ademt_data/cla/')\n",
    "#         else:\n",
    "#             os.system(f'cp ademt_data/{file} ./ademt_data/reg/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ab4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rdkit import Chem\n",
    "\n",
    "def correct_smiles_errors(error_log: str) -> dict:\n",
    "    \"\"\"\n",
    "    从 RDKit 的错误日志中提取并尝试修复无法解析的 SMILES 字符串。\n",
    "\n",
    "    Args:\n",
    "        error_log: 包含了 SMILES 解析错误的日志文本。\n",
    "\n",
    "    Returns:\n",
    "        一个字典，其中键是原始的错误 SMILES 字符串，值是修复后的 SMILES 字符串。\n",
    "        如果一个 SMILES 字符串经过所有尝试后仍然无法被修复，那么对应的值将会是 None。\n",
    "    \"\"\"\n",
    "    # 从日志中提取所有解析失败的 SMILES 字符串\n",
    "    erroneous_smiles_list = re.findall(r\"Failed parsing SMILES '(.*?)'\", error_log)\n",
    "    \n",
    "    corrected_smiles = {}\n",
    "\n",
    "    for smiles in erroneous_smiles_list:\n",
    "        corrected_smiles[smiles] = None  # 默认为未成功修复\n",
    "        \n",
    "        # 尝试一系列的修复方法\n",
    "        # 1. 替换常见的可能引起问题的离子形式\n",
    "        temp_smiles = smiles.replace('[N+H](O)[O-]', '[N+](=O)[O-]')\n",
    "        temp_smiles = temp_smiles.replace('[N+H2]', '[NH2+]')\n",
    "        temp_smiles = temp_smiles.replace('[N+H3]', '[NH3+]')\n",
    "        \n",
    "        # 2. 移除在方括号内明确指定的氢原子，让 RDKit 自动处理\n",
    "        temp_smiles = re.sub(r'\\[([A-Za-z]+)H[0-9]?\\]', r'[\\1]', temp_smiles)\n",
    "        \n",
    "        # 3. 移除特殊官能团周围的括号，以简化结构\n",
    "        temp_smiles = temp_smiles.replace('([N+H](O)[O-])', '[N+](=O)[O-]')\n",
    "        \n",
    "        # 验证修复后的 SMILES 是否可读\n",
    "        mol = Chem.MolFromSmiles(temp_smiles, sanitize=False) # 先不进行化学合理性检查，以尽可能多地解析\n",
    "        if mol is not None:\n",
    "            try:\n",
    "                # 尝试进行化学合理性检查，这是更严格的验证\n",
    "                Chem.SanitizeMol(mol)\n",
    "                corrected_smiles[smiles] = temp_smiles\n",
    "                continue  # 如果成功，则处理下一个 SMILES\n",
    "            except Exception:\n",
    "                # 如果化学合理性检查失败，仍然认为它是一个有效的修复（因为它至少可读）\n",
    "                corrected_smiles[smiles] = temp_smiles\n",
    "                continue\n",
    "\n",
    "    return corrected_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "384fd844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 22:09:21,818 - INFO - 测试诊断功能...\n",
      "2025-08-15 22:09:21,823 - INFO - 原始: 'ClN12([C-]3CC(OC(=O)C(O)(c4ccccc4)c4ccccc4)C[C-]1CC3)[C-2]CC[C-2]2' -> unfixable -> 'None'\n",
      "2025-08-15 22:09:21,823 - INFO -   错误: All repair strategies failed.\n",
      "2025-08-15 22:09:21,823 - INFO - 原始: 'CCO' -> valid -> 'CCO'\n",
      "2025-08-15 22:09:21,824 - INFO - 原始: 'C[N+H3]Cl-' -> unfixable -> 'None'\n",
      "2025-08-15 22:09:21,824 - INFO -   错误: All repair strategies failed.\n",
      "2025-08-15 22:09:21,824 - INFO - 原始: 'Invalid_SMILES' -> unfixable -> 'None'\n",
      "2025-08-15 22:09:21,825 - INFO -   错误: All repair strategies failed.\n",
      "2025-08-15 22:09:21,825 - INFO - 原始: '' -> unfixable -> 'None'\n",
      "2025-08-15 22:09:21,825 - INFO -   错误: Input is not a valid string or is empty.\n",
      "2025-08-15 22:09:21,826 - INFO - --------------------------------------------------\n",
      "2025-08-15 22:09:21,826 - INFO - 开始处理目录 'ademt_data/wash_reg' ...\n",
      "2025-08-15 22:09:21,826 - INFO - 跳过文件: logp.csv\n",
      "2025-08-15 22:09:21,826 - INFO - 跳过文件: pka_acidic.csv\n",
      "2025-08-15 22:09:21,826 - INFO - 正在处理: fu.csv\n",
      "2025-08-15 22:09:21,827 - ERROR - 处理文件 fu.csv 时发生严重错误: 'utf-8' codec can't decode byte 0x83 in position 29: invalid start byte\n",
      "2025-08-15 22:09:21,828 - INFO - 正在处理: cl-plasma.csv\n",
      "2025-08-15 22:09:21,830 - INFO -   诊断和修复 831 个分子...\n",
      "2025-08-15 22:09:21,937 - INFO -   结果: 830 有效, 1 已修复, 0 无法修复\n",
      "2025-08-15 22:09:21,938 - INFO -   生成 831 个分子的InChIKey...\n",
      "2025-08-15 22:09:22,276 - INFO -   成功处理 831 个分子\n",
      "2025-08-15 22:09:22,277 - INFO - 跳过文件: logs.csv\n",
      "2025-08-15 22:09:22,277 - INFO - 正在处理: logvdss.csv\n",
      "2025-08-15 22:09:22,280 - INFO -   诊断和修复 2440 个分子...\n",
      "2025-08-15 22:09:22,590 - INFO -   结果: 2426 有效, 7 已修复, 7 无法修复\n",
      "2025-08-15 22:09:22,592 - INFO -   生成 2433 个分子的InChIKey...\n",
      "2025-08-15 22:09:23,568 - INFO -   成功处理 2433 个分子\n",
      "2025-08-15 22:09:23,569 - INFO - 跳过文件: pka_basic.csv\n",
      "2025-08-15 22:09:23,569 - INFO - 正在处理: t12.csv\n",
      "2025-08-15 22:09:23,576 - INFO -   诊断和修复 3430 个分子...\n",
      "2025-08-15 22:09:24,181 - INFO -   结果: 3323 有效, 107 已修复, 0 无法修复\n",
      "2025-08-15 22:09:24,183 - INFO -   生成 3430 个分子的InChIKey...\n",
      "2025-08-15 22:09:25,854 - INFO -   成功处理 3430 个分子\n",
      "2025-08-15 22:09:25,854 - INFO - 跳过文件: logd.csv\n",
      "2025-08-15 22:09:25,855 - INFO - 正在处理: mdck.csv\n",
      "2025-08-15 22:09:25,857 - INFO -   诊断和修复 1140 个分子...\n",
      "2025-08-15 22:09:26,020 - INFO -   结果: 1140 有效, 0 已修复, 0 无法修复\n",
      "2025-08-15 22:09:26,022 - INFO -   生成 1140 个分子的InChIKey...\n",
      "2025-08-15 22:09:26,524 - INFO -   成功处理 1140 个分子\n",
      "2025-08-15 22:09:26,524 - INFO - 正在处理: caco2.csv\n",
      "2025-08-15 22:09:26,533 - INFO -   诊断和修复 6502 个分子...\n",
      "2025-08-15 22:09:27,584 - INFO -   结果: 6493 有效, 9 已修复, 0 无法修复\n",
      "2025-08-15 22:09:27,586 - INFO -   生成 6502 个分子的InChIKey...\n",
      "2025-08-15 22:09:30,681 - INFO -   成功处理 6502 个分子\n",
      "2025-08-15 22:09:30,682 - INFO - 正在处理: ppb.csv\n",
      "2025-08-15 22:09:30,689 - INFO -   诊断和修复 4712 个分子...\n",
      "2025-08-15 22:09:31,373 - INFO -   结果: 4620 有效, 92 已修复, 0 无法修复\n",
      "2025-08-15 22:09:31,375 - INFO -   生成 4712 个分子的InChIKey...\n",
      "2025-08-15 22:09:33,385 - INFO -   成功处理 4712 个分子\n",
      "2025-08-15 22:09:33,386 - INFO - 正在处理: cl-int.csv\n",
      "2025-08-15 22:09:33,395 - INFO -   诊断和修复 7882 个分子...\n",
      "2025-08-15 22:09:34,594 - INFO -   结果: 7882 有效, 0 已修复, 0 无法修复\n",
      "2025-08-15 22:09:34,597 - INFO -   生成 7882 个分子的InChIKey...\n",
      "2025-08-15 22:09:38,164 - INFO -   成功处理 7882 个分子\n",
      "2025-08-15 22:09:38,164 - INFO - 开始合并所有已处理的数据集...\n",
      "2025-08-15 22:09:38,364 - INFO - 最终合并的数据已保存至: ademt_data/reg1.csv\n",
      "2025-08-15 22:09:38,365 - INFO - 合并后的数据集包含 23198 个唯一分子\n",
      "2025-08-15 22:09:38,365 - INFO - 处理统计:\n",
      "2025-08-15 22:09:38,365 - INFO -   总文件数: 13\n",
      "2025-08-15 22:09:38,365 - INFO -   成功处理: 7\n",
      "2025-08-15 22:09:38,366 - INFO -   跳过文件: 5\n",
      "2025-08-15 22:09:38,366 - INFO -   错误文件: 1\n",
      "2025-08-15 22:09:38,366 - INFO -   总分子数: 26937\n",
      "2025-08-15 22:09:38,366 - INFO -   有效分子: 26714\n",
      "2025-08-15 22:09:38,366 - INFO -   修复分子: 216\n",
      "2025-08-15 22:09:38,367 - INFO -   无法修复: 7\n",
      "2025-08-15 22:09:38,367 - INFO - 发现 7 个无法修复的SMILES\n",
      "2025-08-15 22:09:38,368 - INFO - 无法修复的SMILES报告已保存至: ademt_data/reg1_unfixable_report.csv\n",
      "2025-08-15 22:09:38,368 - INFO - 无法修复的SMILES样例:\n",
      "2025-08-15 22:09:38,368 - INFO -   1. 文件: logvdss.csv, SMILES: OC/C=C\\1/[C@H]2C=3[C@H]4N(c5c(cccc5)[C@@]54[C@@H](...\n",
      "2025-08-15 22:09:38,369 - INFO -   2. 文件: logvdss.csv, SMILES: OC/C=C/1\\[C@H]2C=3[C@@H]4N(c5c(cccc5)[C@@]54[C@H](...\n",
      "2025-08-15 22:09:38,369 - INFO -   3. 文件: logvdss.csv, SMILES: O=C1[C@@H]2Oc3c(O)ccc4c3[C@]32[C@](O)([C@H]([N+@@]...\n",
      "2025-08-15 22:09:38,369 - INFO -   4. 文件: logvdss.csv, SMILES: OC/C=C/1\\[C@H]2C=3[C@@H]4N(c5c(cccc5)[C@@]54[C@@H]...\n",
      "2025-08-15 22:09:38,369 - INFO -   5. 文件: logvdss.csv, SMILES: O=C1[C@@H]2Oc3c(O)ccc4c3[C@@]32[C@](O)([C@H]([N+@]...\n",
      "2025-08-15 22:09:38,369 - INFO -   6. 文件: logvdss.csv, SMILES: OC/C=C/1\\[C@H]2C=3[C@@H]4N(c5c(cccc5)[C@@]54[C@@H]...\n",
      "2025-08-15 22:09:38,370 - INFO -   7. 文件: logvdss.csv, SMILES: O=C1N2[C@@H]([C@@H]3[C@H]4[N+@]([O-])(CCC3)CCC[C@H...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from rdkit import Chem\n",
    "from rdkit import rdBase\n",
    "from functools import reduce\n",
    "import logging\n",
    "\n",
    "# 禁用RDKit的详细错误日志，以便我们自己捕获和处理\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 核心辅助函数 (带有诊断功能) ---\n",
    "\n",
    "def smiles_to_inchikey(smiles_list):\n",
    "    \"\"\"将SMILES列表转换为InChIKey列表。\"\"\"\n",
    "    inchikeys = []\n",
    "    for smiles in smiles_list:\n",
    "        if pd.isna(smiles) or smiles is None:\n",
    "            inchikeys.append(None)\n",
    "            continue\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                inchikey = Chem.MolToInchiKey(mol)\n",
    "                inchikeys.append(inchikey)\n",
    "            else:\n",
    "                inchikeys.append(None)\n",
    "        except Exception:\n",
    "            inchikeys.append(None)\n",
    "    return inchikeys\n",
    "\n",
    "\n",
    "def diagnose_and_fix_smiles(smiles: str) -> dict:\n",
    "    \"\"\"\n",
    "    诊断并尝试修复单个SMILES字符串。\n",
    "\n",
    "    Returns:\n",
    "        一个包含诊断结果的字典:\n",
    "        {'status': 'valid'/'fixed'/'unfixable', 'smiles': str/None, 'error': str/None}\n",
    "    \"\"\"\n",
    "    if not isinstance(smiles, str) or not smiles.strip():\n",
    "        return {'status': 'unfixable', 'smiles': None, 'error': 'Input is not a valid string or is empty.'}\n",
    "\n",
    "    smiles = smiles.strip()\n",
    "\n",
    "    # 1. 尝试原始SMILES\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol is not None:\n",
    "            return {'status': 'valid', 'smiles': smiles, 'error': None}\n",
    "    except Exception as e:\n",
    "        original_error = str(e)\n",
    "\n",
    "    # 2. 应用一系列修复策略\n",
    "    smiles_fixed = smiles\n",
    "    \n",
    "    # 策略 A: 处理常见的元素符号错误\n",
    "    element_typo_replacements = {\n",
    "        'IN': '[In]',\n",
    "        'SN': '[Sn]',\n",
    "        'PB': '[Pb]',\n",
    "        'AS': '[As]',\n",
    "        'SB': '[Sb]',\n",
    "        'BI': '[Bi]'\n",
    "    }\n",
    "    \n",
    "    for typo, correction in element_typo_replacements.items():\n",
    "        # 使用更精确的正则表达式匹配\n",
    "        pattern = r'(?<![A-Za-z\\[\\]])' + re.escape(typo) + r'(?![A-Za-z\\]\\+\\-])'\n",
    "        smiles_fixed = re.sub(pattern, correction, smiles_fixed)\n",
    "\n",
    "    # 策略 B: 转换 [N+H] 风格为 [NH+] 风格\n",
    "    charge_hydrogen_format_replacements = {\n",
    "        r'\\[N\\+H\\]': '[NH+]', \n",
    "        r'\\[N\\+H2\\]': '[NH2+]', \n",
    "        r'\\[N\\+H3\\]': '[NH3+]', \n",
    "        r'\\[n\\+H\\]': '[nH+]', \n",
    "        r'\\[O\\+H\\]': '[OH+]',\n",
    "        r'\\[S\\+H\\]': '[SH+]'\n",
    "    }\n",
    "    \n",
    "    for pattern, replacement in charge_hydrogen_format_replacements.items():\n",
    "        smiles_fixed = re.sub(pattern, replacement, smiles_fixed)\n",
    "    \n",
    "    # 策略 C: 修复不平衡的电荷（移除不必要的显式氢原子）\n",
    "    # 匹配 [元素+电荷H数字] 格式并简化\n",
    "    smiles_fixed = re.sub(r'\\[([A-Za-z@\\*]+[+\\-]\\d*)H\\d*\\]', r'[\\1]', smiles_fixed)\n",
    "\n",
    "    # 策略 D: 处理可能的括号不匹配问题\n",
    "    # 统计括号数量\n",
    "    open_brackets = smiles_fixed.count('(')\n",
    "    close_brackets = smiles_fixed.count(')')\n",
    "    if open_brackets != close_brackets:\n",
    "        # 尝试平衡括号\n",
    "        if open_brackets > close_brackets:\n",
    "            smiles_fixed += ')' * (open_brackets - close_brackets)\n",
    "        elif close_brackets > open_brackets:\n",
    "            smiles_fixed = '(' * (close_brackets - open_brackets) + smiles_fixed\n",
    "\n",
    "    # 策略 E: 处理方括号不匹配\n",
    "    open_square = smiles_fixed.count('[')\n",
    "    close_square = smiles_fixed.count(']')\n",
    "    if open_square != close_square:\n",
    "        if open_square > close_square:\n",
    "            smiles_fixed += ']' * (open_square - close_square)\n",
    "        elif close_square > open_square:\n",
    "            smiles_fixed = '[' * (close_square - open_square) + smiles_fixed\n",
    "\n",
    "    # 策略 F: 处理特殊的带电离子表示法问题\n",
    "    # 修复 [C-] 这样可能有问题的离子表示\n",
    "    smiles_fixed = re.sub(r'\\[C-\\]', '[C-]', smiles_fixed)  # 这个例子可能不会改变，但保留模式\n",
    "    \n",
    "    # 策略 G: 处理连续的电荷符号\n",
    "    smiles_fixed = re.sub(r'([+\\-])\\1+', r'\\1', smiles_fixed)\n",
    "\n",
    "    # 3. 尝试解析修复后的SMILES\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles_fixed, sanitize=True)\n",
    "        if mol is not None:\n",
    "            # 验证修复的分子是否合理\n",
    "            try:\n",
    "                # 尝试标准化SMILES来验证\n",
    "                canonical_smiles = Chem.MolToSmiles(mol)\n",
    "                if canonical_smiles:\n",
    "                    return {'status': 'fixed', 'smiles': smiles_fixed, 'error': None}\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            # 如果标准化失败但分子对象有效，仍然返回修复状态\n",
    "            if smiles_fixed != smiles:\n",
    "                return {'status': 'fixed', 'smiles': smiles_fixed, 'error': None}\n",
    "            else:\n",
    "                return {'status': 'valid', 'smiles': smiles, 'error': None}\n",
    "    except Exception as e:\n",
    "        final_error = str(e)\n",
    "        return {'status': 'unfixable', 'smiles': None, 'error': f\"Original: {original_error if 'original_error' in locals() else 'Unknown'}, After fix: {final_error}\"}\n",
    "\n",
    "    # 如果所有修复尝试都失败\n",
    "    return {'status': 'unfixable', 'smiles': None, 'error': 'All repair strategies failed.'}\n",
    "\n",
    "\n",
    "def validate_dataframe(df, filename):\n",
    "    \"\"\"验证数据框的基本结构\"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(f\"文件 {filename} 是空的\")\n",
    "        return False\n",
    "    \n",
    "    if 'smiles' not in df.columns:\n",
    "        logger.warning(f\"文件 {filename} 缺少 'smiles' 列\")\n",
    "        return False\n",
    "    \n",
    "    if 'Label' not in df.columns:\n",
    "        logger.warning(f\"文件 {filename} 缺少 'Label' 列\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def process_and_merge_datasets_with_diagnostics(input_dir, output_path, files_to_skip=None, report_unfixable=True):\n",
    "    \"\"\"\n",
    "    加载、修复、诊断、清洗并合并CSV文件。\n",
    "    \"\"\"\n",
    "    if files_to_skip is None:\n",
    "        files_to_skip = []\n",
    "    \n",
    "    skip_list_lower = [item.lower().strip() for item in files_to_skip]\n",
    "    all_processed_dfs = []\n",
    "    unfixable_report = []\n",
    "    processing_stats = {\n",
    "        'total_files': 0,\n",
    "        'processed_files': 0,\n",
    "        'skipped_files': 0,\n",
    "        'error_files': 0,\n",
    "        'total_molecules': 0,\n",
    "        'valid_molecules': 0,\n",
    "        'fixed_molecules': 0,\n",
    "        'unfixable_molecules': 0\n",
    "    }\n",
    "\n",
    "    logger.info(f\"开始处理目录 '{input_dir}' ...\")\n",
    "    \n",
    "    if not os.path.exists(input_dir):\n",
    "        logger.error(f\"输入目录不存在: {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "    processing_stats['total_files'] = len(csv_files)\n",
    "    \n",
    "    for filename in csv_files:\n",
    "        task_name_original = filename.split('.')[0]\n",
    "        task_name_normalized = task_name_original.strip().lower()\n",
    "\n",
    "        if task_name_normalized in skip_list_lower:\n",
    "            logger.info(f\"跳过文件: {filename}\")\n",
    "            processing_stats['skipped_files'] += 1\n",
    "            continue\n",
    "            \n",
    "        logger.info(f\"正在处理: {filename}\")\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # 标准化列名\n",
    "            if 'SMILES' in df.columns:\n",
    "                df.rename(columns={'SMILES': 'smiles'}, inplace=True)\n",
    "            \n",
    "            # 验证数据框\n",
    "            if not validate_dataframe(df, filename):\n",
    "                processing_stats['error_files'] += 1\n",
    "                continue\n",
    "\n",
    "            original_count = len(df)\n",
    "            processing_stats['total_molecules'] += original_count\n",
    "\n",
    "            # 应用诊断和修复函数\n",
    "            logger.info(f\"  诊断和修复 {original_count} 个分子...\")\n",
    "            diagnostics = df['smiles'].apply(diagnose_and_fix_smiles)\n",
    "            \n",
    "            df['smiles_fixed'] = diagnostics.apply(lambda x: x['smiles'])\n",
    "            df['status'] = diagnostics.apply(lambda x: x['status'])\n",
    "            df['error_msg'] = diagnostics.apply(lambda x: x['error'])\n",
    "\n",
    "            # 统计结果\n",
    "            status_counts = df['status'].value_counts()\n",
    "            processing_stats['valid_molecules'] += status_counts.get('valid', 0)\n",
    "            processing_stats['fixed_molecules'] += status_counts.get('fixed', 0)\n",
    "            processing_stats['unfixable_molecules'] += status_counts.get('unfixable', 0)\n",
    "\n",
    "            logger.info(f\"  结果: {status_counts.get('valid', 0)} 有效, {status_counts.get('fixed', 0)} 已修复, {status_counts.get('unfixable', 0)} 无法修复\")\n",
    "\n",
    "            # 收集无法修复的SMILES报告\n",
    "            if report_unfixable:\n",
    "                unfixable_df = df[df['status'] == 'unfixable']\n",
    "                for _, row in unfixable_df.iterrows():\n",
    "                    unfixable_report.append({\n",
    "                        'file': filename,\n",
    "                        'original_smiles': row['smiles'],\n",
    "                        'error': row['error_msg']\n",
    "                    })\n",
    "\n",
    "            # 清洗数据：只保留有效和已修复的分子\n",
    "            cleaned_df = df[df['status'].isin(['valid', 'fixed'])].copy()\n",
    "            \n",
    "            if cleaned_df.empty:\n",
    "                logger.warning(f\"  文件 {filename} 清洗后没有有效数据\")\n",
    "                continue\n",
    "\n",
    "            # 生成InChIKey\n",
    "            logger.info(f\"  生成 {len(cleaned_df)} 个分子的InChIKey...\")\n",
    "            cleaned_df['Inchikey'] = smiles_to_inchikey(cleaned_df['smiles_fixed'])\n",
    "            \n",
    "            # 移除无法生成InChIKey的行\n",
    "            cleaned_df = cleaned_df.dropna(subset=['Inchikey'])\n",
    "            \n",
    "            if cleaned_df.empty:\n",
    "                logger.warning(f\"  文件 {filename} 生成InChIKey后没有有效数据\")\n",
    "                continue\n",
    "\n",
    "            # 准备最终数据框\n",
    "            cleaned_df.rename(columns={'Label': task_name_original}, inplace=True)\n",
    "            final_df = cleaned_df[['smiles_fixed', 'Inchikey', task_name_original]].copy()\n",
    "            final_df.rename(columns={'smiles_fixed': 'smiles'}, inplace=True)\n",
    "            \n",
    "            all_processed_dfs.append(final_df)\n",
    "            processing_stats['processed_files'] += 1\n",
    "            \n",
    "            logger.info(f\"  成功处理 {len(final_df)} 个分子\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"处理文件 {filename} 时发生严重错误: {e}\")\n",
    "            processing_stats['error_files'] += 1\n",
    "\n",
    "    # 合并数据\n",
    "    if all_processed_dfs:\n",
    "        logger.info(\"开始合并所有已处理的数据集...\")\n",
    "        try:\n",
    "            merged_df = reduce(\n",
    "                lambda left, right: pd.merge(left, right, on=['smiles', 'Inchikey'], how='outer'), \n",
    "                all_processed_dfs\n",
    "            )\n",
    "            \n",
    "            # 创建输出目录（如果不存在）\n",
    "            output_dir = os.path.dirname(output_path)\n",
    "            if output_dir and not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            merged_df.to_csv(output_path, index=False)\n",
    "            logger.info(f\"最终合并的数据已保存至: {output_path}\")\n",
    "            logger.info(f\"合并后的数据集包含 {len(merged_df)} 个唯一分子\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"合并数据时发生错误: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"没有可合并的数据\")\n",
    "\n",
    "    # 打印处理统计\n",
    "    logger.info(\"处理统计:\")\n",
    "    logger.info(f\"  总文件数: {processing_stats['total_files']}\")\n",
    "    logger.info(f\"  成功处理: {processing_stats['processed_files']}\")\n",
    "    logger.info(f\"  跳过文件: {processing_stats['skipped_files']}\")\n",
    "    logger.info(f\"  错误文件: {processing_stats['error_files']}\")\n",
    "    logger.info(f\"  总分子数: {processing_stats['total_molecules']}\")\n",
    "    logger.info(f\"  有效分子: {processing_stats['valid_molecules']}\")\n",
    "    logger.info(f\"  修复分子: {processing_stats['fixed_molecules']}\")\n",
    "    logger.info(f\"  无法修复: {processing_stats['unfixable_molecules']}\")\n",
    "\n",
    "    # 保存无法修复的SMILES报告\n",
    "    if unfixable_report:\n",
    "        logger.info(f\"发现 {len(unfixable_report)} 个无法修复的SMILES\")\n",
    "        report_df = pd.DataFrame(unfixable_report)\n",
    "        \n",
    "        # 保存报告到文件\n",
    "        report_path = output_path.replace('.csv', '_unfixable_report.csv')\n",
    "        report_df.to_csv(report_path, index=False)\n",
    "        logger.info(f\"无法修复的SMILES报告已保存至: {report_path}\")\n",
    "        \n",
    "        if len(unfixable_report) <= 20:  # 只在控制台显示少量记录\n",
    "            logger.info(\"无法修复的SMILES样例:\")\n",
    "            for i, record in enumerate(unfixable_report[:10]):\n",
    "                logger.info(f\"  {i+1}. 文件: {record['file']}, SMILES: {record['original_smiles'][:50]}...\")\n",
    "\n",
    "\n",
    "# --- 测试和执行 ---\n",
    "if __name__ == '__main__':\n",
    "    # 测试新的诊断函数\n",
    "    logger.info(\"测试诊断功能...\")\n",
    "    test_cases = [\n",
    "        'ClN12([C-]3CC(OC(=O)C(O)(c4ccccc4)c4ccccc4)C[C-]1CC3)[C-2]CC[C-2]2',\n",
    "        'CCO',  # 简单有效的SMILES\n",
    "        'C[N+H3]Cl-',  # 需要修复的格式\n",
    "        'Invalid_SMILES',  # 无效的SMILES\n",
    "        '',  # 空字符串\n",
    "    ]\n",
    "    \n",
    "    for test_smiles in test_cases:\n",
    "        result = diagnose_and_fix_smiles(test_smiles)\n",
    "        logger.info(f\"原始: '{test_smiles}' -> {result['status']} -> '{result['smiles']}'\")\n",
    "        if result['error']:\n",
    "            logger.info(f\"  错误: {result['error']}\")\n",
    "    \n",
    "    logger.info(\"-\" * 50)\n",
    "    \n",
    "    # 运行主流程\n",
    "    INPUT_REG_DIR = 'ademt_data/wash_reg'\n",
    "    OUTPUT_REG_FILE = 'ademt_data/reg1.csv'\n",
    "    SKIP_FILES = ['logD', 'logP', 'logS', 'pka_acidic', 'pka_basic']\n",
    "\n",
    "    # 取消注释以运行主处理流程\n",
    "    process_and_merge_datasets_with_diagnostics(INPUT_REG_DIR, OUTPUT_REG_FILE, SKIP_FILES, report_unfixable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b3624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import re\n",
    "# from rdkit import Chem\n",
    "# from rdkit import rdBase\n",
    "# from functools import reduce\n",
    "\n",
    "# # 禁用RDKit的详细错误日志，以便我们自己捕获和处理\n",
    "# rdBase.DisableLog('rdApp.error')\n",
    "\n",
    "# # --- 核心辅助函数 (带有诊断功能) ---\n",
    "\n",
    "# def smiles_to_inchikey(smiles_list):\n",
    "#     \"\"\"将SMILES列表转换为InChIKey列表。\"\"\"\n",
    "#     # ... (代码与之前相同) ...\n",
    "#     inchikeys = []\n",
    "#     for smiles in smiles_list:\n",
    "#         if pd.isna(smiles):\n",
    "#             inchikeys.append(None)\n",
    "#             continue\n",
    "#         mol = Chem.MolFromSmiles(smiles)\n",
    "#         if mol is not None:\n",
    "#             inchikey = Chem.MolToInchiKey(mol)\n",
    "#             inchikeys.append(inchikey)\n",
    "#         else:\n",
    "#             inchikeys.append(None)\n",
    "#     return inchikeys\n",
    "\n",
    "\n",
    "# def diagnose_and_fix_smiles(smiles: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     诊断并尝试修复单个SMILES字符串。\n",
    "\n",
    "#     Returns:\n",
    "#         一个包含诊断结果的字典:\n",
    "#         {'status': 'valid'/'fixed'/'unfixable', 'smiles': str/None, 'error': str/None}\n",
    "#     \"\"\"\n",
    "#     if not isinstance(smiles, str) or not smiles:\n",
    "#         return {'status': 'unfixable', 'smiles': None, 'error': 'Input is not a valid string.'}\n",
    "\n",
    "#     # 1. 尝试原始SMILES\n",
    "#     try:\n",
    "#         if Chem.MolFromSmiles(smiles, sanitize=True) is None:\n",
    "#             return {'status': 'valid', 'smiles': smiles, 'error': None}\n",
    "#     except Exception as e:\n",
    "#         pass # 继续尝试修复\n",
    "\n",
    "#     # 2. 应用一系列修复策略\n",
    "#     smiles_fixed = smiles\n",
    "    \n",
    "#     # 策略 A: 处理常见的元素符号错误\n",
    "#     element_typo_replacements = {'IN': '[In]'}\n",
    "#     for typo, correction in element_typo_replacements.items():\n",
    "#         smiles_fixed = re.sub(r'(^|[^a-zA-Z])' + re.escape(typo) + r'($|[^a-zA-Z])', r'\\1' + correction + r'\\2', smiles_fixed)\n",
    "\n",
    "#     # 策略 B: 转换 [N+H] 风格为 [NH+] 风格\n",
    "#     charge_hydrogen_format_replacements = {\n",
    "#         '[N+H]': '[NH+]', '[N+H2]': '[NH2+]', '[N+H3]': '[NH3+]', '[n+H]': '[nH+]', '[O+H]': '[OH+]'\n",
    "#     }\n",
    "#     for old, new in charge_hydrogen_format_replacements.items():\n",
    "#         smiles_fixed = smiles_fixed.replace(old, new)\n",
    "    \n",
    "#     # 策略 C: 移除其他带电原子中不必要的显式氢原子\n",
    "#     smiles_fixed = re.sub(r'\\[([A-Za-z@\\*]+[+\\-]\\d+)H\\d*\\]', r'[\\1]', smiles_fixed)\n",
    "\n",
    "#     # 3. 尝试解析修复后的SMILES并捕获错误\n",
    "#     try:\n",
    "#         mol = Chem.MolFromSmiles(smiles_fixed, sanitize=True)\n",
    "#         if mol is not None:\n",
    "#             # 检查修复是否真的改变了字符串\n",
    "#             if smiles_fixed != smiles:\n",
    "#                 return {'status': 'fixed', 'smiles': smiles_fixed, 'error': None}\n",
    "#             else:\n",
    "#                 # 这种情况理论上不应该发生，因为原始的已经检查过了\n",
    "#                 return {'status': 'valid', 'smiles': smiles, 'error': None}\n",
    "#     except Exception as e:\n",
    "#         # 如果修复后仍然失败，捕获错误信息\n",
    "#         return {'status': 'unfixable', 'smiles': None, 'error': str(e)}\n",
    "\n",
    "#     # 如果修复后没有变化且原始SMILES无效\n",
    "#     return {'status': 'unfixable', 'smiles': None, 'error': 'Initial parsing failed and no fixes were applicable.'}\n",
    "\n",
    "\n",
    "# # --- 主处理流程 ---\n",
    "\n",
    "# def process_and_merge_datasets_with_diagnostics(input_dir, output_path, files_to_skip, report_unfixable=True):\n",
    "#     \"\"\"\n",
    "#     加载、修复、诊断、清洗并合并CSV文件。\n",
    "#     \"\"\"\n",
    "#     skip_list_lower = [item.lower().strip() for item in files_to_skip]\n",
    "#     all_processed_dfs = []\n",
    "#     unfixable_report = []\n",
    "\n",
    "#     print(f\"开始处理目录 '{input_dir}' ...\")\n",
    "    \n",
    "#     for filename in os.listdir(input_dir):\n",
    "#         if not filename.endswith('.csv'): continue\n",
    "        \n",
    "#         task_name_original = filename.split('.')[0]\n",
    "#         task_name_normalized = task_name_original.strip().lower()\n",
    "\n",
    "#         if task_name_normalized in skip_list_lower: continue\n",
    "            \n",
    "#         print(f\"\\n--- 正在处理: {filename} ---\")\n",
    "#         file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "#         try:\n",
    "#             df = pd.read_csv(file_path)\n",
    "#             if 'SMILES' in df.columns: df.rename(columns={'SMILES': 'smiles'}, inplace=True)\n",
    "#             if 'smiles' not in df.columns or 'Label' not in df.columns: continue\n",
    "\n",
    "#             # 应用诊断和修复函数\n",
    "#             diagnostics = df['smiles'].apply(diagnose_and_fix_smiles)\n",
    "#             df['smiles_fixed'] = diagnostics.apply(lambda x: x['smiles'])\n",
    "#             df['status'] = diagnostics.apply(lambda x: x['status'])\n",
    "#             df['error_msg'] = diagnostics.apply(lambda x: x['error'])\n",
    "\n",
    "#             # 报告无法修复的SMILES\n",
    "#             if report_unfixable:\n",
    "#                 unfixable_df = df[df['status'] == 'unfixable']\n",
    "#                 for _, row in unfixable_df.iterrows():\n",
    "#                     unfixable_report.append({\n",
    "#                         'file': filename,\n",
    "#                         'original_smiles': row['smiles'],\n",
    "#                         'error': row['error_msg']\n",
    "#                     })\n",
    "\n",
    "#             # 清洗数据\n",
    "#             cleaned_df = df.dropna(subset=['smiles_fixed']).copy()\n",
    "#             if cleaned_df.empty: continue\n",
    "\n",
    "#             # 后续处理...\n",
    "#             cleaned_df['Inchikey'] = smiles_to_inchikey(cleaned_df['smiles_fixed'])\n",
    "#             cleaned_df.rename(columns={'Label': task_name_original}, inplace=True)\n",
    "#             final_df = cleaned_df[['smiles_fixed', 'Inchikey', task_name_original]]\n",
    "#             final_df.rename(columns={'smiles_fixed': 'smiles'}, inplace=True)\n",
    "#             all_processed_dfs.append(final_df)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"  -> 处理文件 {filename} 时发生严重错误: {e}\")\n",
    "\n",
    "#     # 合并数据...\n",
    "#     if all_processed_dfs:\n",
    "#         print(\"\\n--- 开始合并所有已处理的数据集 ---\")\n",
    "#         merged_df = reduce(lambda left, right: pd.merge(left, right, on=['smiles', 'Inchikey'], how='outer'), all_processed_dfs)\n",
    "#         merged_df.to_csv(output_path, index=False)\n",
    "#         print(f\"最终合并的数据已保存至: {output_path}\")\n",
    "#     else:\n",
    "#         print(\"\\n没有可合并的数据。\")\n",
    "\n",
    "#     # 打印无法修复的SMILES报告\n",
    "#     if unfixable_report:\n",
    "#         print(\"\\n--- 无法修复的SMILES报告 ---\")\n",
    "#         report_df = pd.DataFrame(unfixable_report)\n",
    "#         print(report_df.to_string())\n",
    "\n",
    "\n",
    "# # --- 执行 ---\n",
    "# if __name__ == '__main__':\n",
    "#     # --- 测试新的诊断函数 ---\n",
    "#     print(\"--- 测试诊断功能 ---\")\n",
    "#     test_smiles = 'BrN([C-2]C)([C-2]C)([C-2]C)[C-2]C'\n",
    "#     result = diagnose_and_fix_smiles(test_smiles)\n",
    "#     print(f\"原始SMILES: {test_smiles}\")\n",
    "#     print(f\"诊断结果: {result}\")\n",
    "#     print(\"-\" * 20)\n",
    "    \n",
    "#     # --- 运行您的主流程 ---\n",
    "#     INPUT_REG_DIR = 'ademt_data/wash_reg'\n",
    "#     OUTPUT_REG_FILE = 'ademt_data/reg.csv'\n",
    "#     # SKIP_FILES = ['logD', 'logP', 'logS', 'pka_acidic', 'pka_basic']\n",
    "#     SKIP_FILES = []\n",
    "\n",
    "#     process_and_merge_datasets_with_diagnostics(INPUT_REG_DIR, OUTPUT_REG_FILE, SKIP_FILES, report_unfixable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c6ae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 测试诊断功能 ---\n",
      "原始SMILES: BrN([C-2]C)([C-2]C)([C-2]C)[C-2]C\n",
      "诊断结果: {'status': 'unfixable', 'smiles': None, 'error': 'Initial parsing failed and no fixes were applicable.'}\n",
      "--------------------------------------------------\n",
      "开始处理目录 'ademt_data/wash_reg' ...\n",
      "分割比例: 训练集 80.0%, 验证集 10.0%, 测试集 10.0%\n",
      "\n",
      "--- 正在处理: logp.csv ---\n",
      "  原始数据: 12682 样本\n",
      "  清洗后数据: 12635 样本\n",
      "  最终数据: 12635 样本\n",
      "  分割结果: 训练集 10107, 验证集 1264, 测试集 1264\n",
      "\n",
      "--- 正在处理: pka_acidic.csv ---\n",
      "  原始数据: 2750 样本\n",
      "  清洗后数据: 2750 样本\n",
      "  最终数据: 2750 样本\n",
      "  分割结果: 训练集 2200, 验证集 275, 测试集 275\n",
      "\n",
      "--- 正在处理: fu.csv ---\n",
      "  -> 处理文件 fu.csv 时发生严重错误: 'utf-8' codec can't decode byte 0x83 in position 29: invalid start byte\n",
      "\n",
      "--- 正在处理: cl-plasma.csv ---\n",
      "  原始数据: 831 样本\n",
      "  清洗后数据: 831 样本\n",
      "  最终数据: 831 样本\n",
      "  分割结果: 训练集 664, 验证集 83, 测试集 84\n",
      "\n",
      "--- 正在处理: logs.csv ---\n",
      "  原始数据: 4797 样本\n",
      "  清洗后数据: 4797 样本\n",
      "  最终数据: 4797 样本\n",
      "  分割结果: 训练集 3837, 验证集 480, 测试集 480\n",
      "\n",
      "--- 正在处理: logvdss.csv ---\n",
      "  原始数据: 2440 样本\n",
      "  清洗后数据: 2433 样本\n",
      "  最终数据: 2433 样本\n",
      "  分割结果: 训练集 1945, 验证集 244, 测试集 244\n",
      "\n",
      "--- 正在处理: pka_basic.csv ---\n",
      "  原始数据: 2992 样本\n",
      "  清洗后数据: 2992 样本\n",
      "  最终数据: 2992 样本\n",
      "  分割结果: 训练集 2392, 验证集 300, 测试集 300\n",
      "\n",
      "--- 正在处理: t12.csv ---\n",
      "  原始数据: 3430 样本\n",
      "  清洗后数据: 3430 样本\n",
      "  最终数据: 3430 样本\n",
      "  分割结果: 训练集 2744, 验证集 343, 测试集 343\n",
      "\n",
      "--- 正在处理: logd.csv ---\n",
      "  原始数据: 19155 样本\n",
      "  清洗后数据: 19107 样本\n",
      "  最终数据: 19107 样本\n",
      "  分割结果: 训练集 15285, 验证集 1911, 测试集 1911\n",
      "\n",
      "--- 正在处理: mdck.csv ---\n",
      "  原始数据: 1140 样本\n",
      "  清洗后数据: 1140 样本\n",
      "  最终数据: 1140 样本\n",
      "  分割结果: 训练集 911, 验证集 115, 测试集 114\n",
      "\n",
      "--- 正在处理: caco2.csv ---\n",
      "  原始数据: 6502 样本\n",
      "  清洗后数据: 6502 样本\n",
      "  最终数据: 6502 样本\n",
      "  分割结果: 训练集 5200, 验证集 651, 测试集 651\n",
      "\n",
      "--- 正在处理: ppb.csv ---\n",
      "  原始数据: 4712 样本\n",
      "  清洗后数据: 4706 样本\n",
      "  最终数据: 4706 样本\n",
      "  分割结果: 训练集 3764, 验证集 471, 测试集 471\n",
      "\n",
      "--- 正在处理: cl-int.csv ---\n",
      "  原始数据: 7882 样本\n",
      "  清洗后数据: 7882 样本\n",
      "  最终数据: 7882 样本\n",
      "  分割结果: 训练集 6304, 验证集 789, 测试集 789\n",
      "\n",
      "--- 合并所有任务的数据集 ---\n",
      "  训练集: 合并 12 个任务的数据\n",
      "    合并后: 47380 个唯一分子\n",
      "    保存至: ademt_data/reg_3/train.csv\n",
      "  验证集: 合并 12 个任务的数据\n",
      "    合并后: 6766 个唯一分子\n",
      "    保存至: ademt_data/reg_3/val.csv\n",
      "  测试集: 合并 12 个任务的数据\n",
      "    合并后: 6758 个唯一分子\n",
      "    保存至: ademt_data/reg_3/test.csv\n",
      "\n",
      "--- 分割统计报告 ---\n",
      "      task  total  train  val  test  train_pct   val_pct  test_pct\n",
      "      logp  12635  10107 1264  1264  79.992085 10.003957 10.003957\n",
      "pka_acidic   2750   2200  275   275  80.000000 10.000000 10.000000\n",
      " cl-plasma    831    664   83    84  79.903730  9.987966 10.108303\n",
      "      logs   4797   3837  480   480  79.987492 10.006254 10.006254\n",
      "   logvdss   2433   1945  244   244  79.942458 10.028771 10.028771\n",
      " pka_basic   2992   2392  300   300  79.946524 10.026738 10.026738\n",
      "       t12   3430   2744  343   343  80.000000 10.000000 10.000000\n",
      "      logd  19107  15285 1911  1911  79.996860 10.001570 10.001570\n",
      "      mdck   1140    911  115   114  79.912281 10.087719 10.000000\n",
      "     caco2   6502   5200  651   651  79.975392 10.012304 10.012304\n",
      "       ppb   4706   3764  471   471  79.983000 10.008500 10.008500\n",
      "    cl-int   7882   6304  789   789  79.979701 10.010150 10.010150\n",
      "详细统计已保存至: ademt_data/reg_3/split_summary.csv\n",
      "\n",
      "--- 生成完整合并数据集 ---\n",
      "完整数据集: 57261 个分子，保存至: ademt_data/reg_3/complete_dataset.csv\n",
      "\n",
      "--- 无法修复的SMILES报告 ---\n",
      "发现 108 个无法修复的SMILES\n",
      "详细报告已保存至: ademt_data/reg_3/unfixable_smiles_report.csv\n",
      "\n",
      "=== 处理完成 ===\n",
      "所有结果保存在目录: ademt_data/reg_3\n",
      "  - train.csv: 合并的训练集\n",
      "  - val.csv: 合并的验证集\n",
      "  - test.csv: 合并的测试集\n",
      "  - complete_dataset.csv: 完整合并数据集\n",
      "  - individual_tasks/: 各任务的单独分割结果\n",
      "  - split_summary.csv: 分割统计报告\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from rdkit import Chem\n",
    "from rdkit import rdBase\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 禁用RDKit的详细错误日志，以便我们自己捕获和处理\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "\n",
    "# --- 核心辅助函数 (带有诊断功能) ---\n",
    "\n",
    "def smiles_to_inchikey(smiles_list):\n",
    "    \"\"\"将SMILES列表转换为InChIKey列表。\"\"\"\n",
    "    inchikeys = []\n",
    "    for smiles in smiles_list:\n",
    "        if pd.isna(smiles):\n",
    "            inchikeys.append(None)\n",
    "            continue\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            inchikey = Chem.MolToInchiKey(mol)\n",
    "            inchikeys.append(inchikey)\n",
    "        else:\n",
    "            inchikeys.append(None)\n",
    "    return inchikeys\n",
    "\n",
    "\n",
    "def diagnose_and_fix_smiles(smiles: str) -> dict:\n",
    "    \"\"\"\n",
    "    诊断并尝试修复单个SMILES字符串。\n",
    "\n",
    "    Returns:\n",
    "        一个包含诊断结果的字典:\n",
    "        {'status': 'valid'/'fixed'/'unfixable', 'smiles': str/None, 'error': str/None}\n",
    "    \"\"\"\n",
    "    if not isinstance(smiles, str) or not smiles:\n",
    "        return {'status': 'unfixable', 'smiles': None, 'error': 'Input is not a valid string.'}\n",
    "\n",
    "    # 1. 尝试原始SMILES\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol is not None:\n",
    "            return {'status': 'valid', 'smiles': smiles, 'error': None}\n",
    "    except Exception as e:\n",
    "        pass # 继续尝试修复\n",
    "\n",
    "    # 2. 应用一系列修复策略\n",
    "    smiles_fixed = smiles\n",
    "    \n",
    "    # 策略 A: 处理常见的元素符号错误\n",
    "    element_typo_replacements = {'IN': '[In]'}\n",
    "    for typo, correction in element_typo_replacements.items():\n",
    "        smiles_fixed = re.sub(r'(^|[^a-zA-Z])' + re.escape(typo) + r'($|[^a-zA-Z])', r'\\1' + correction + r'\\2', smiles_fixed)\n",
    "\n",
    "    # 策略 B: 转换 [N+H] 风格为 [NH+] 风格\n",
    "    charge_hydrogen_format_replacements = {\n",
    "        '[N+H]': '[NH+]', '[N+H2]': '[NH2+]', '[N+H3]': '[NH3+]', '[n+H]': '[nH+]', '[O+H]': '[OH+]'\n",
    "    }\n",
    "    for old, new in charge_hydrogen_format_replacements.items():\n",
    "        smiles_fixed = smiles_fixed.replace(old, new)\n",
    "    \n",
    "    # 策略 C: 移除其他带电原子中不必要的显式氢原子\n",
    "    smiles_fixed = re.sub(r'\\[([A-Za-z@\\*]+[+\\-]\\d+)H\\d*\\]', r'[\\1]', smiles_fixed)\n",
    "\n",
    "    # 3. 尝试解析修复后的SMILES并捕获错误\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles_fixed, sanitize=True)\n",
    "        if mol is not None:\n",
    "            # 检查修复是否真的改变了字符串\n",
    "            if smiles_fixed != smiles:\n",
    "                return {'status': 'fixed', 'smiles': smiles_fixed, 'error': None}\n",
    "            else:\n",
    "                # 这种情况理论上不应该发生，因为原始的已经检查过了\n",
    "                return {'status': 'valid', 'smiles': smiles, 'error': None}\n",
    "    except Exception as e:\n",
    "        # 如果修复后仍然失败，捕获错误信息\n",
    "        return {'status': 'unfixable', 'smiles': None, 'error': str(e)}\n",
    "\n",
    "    # 如果修复后没有变化且原始SMILES无效\n",
    "    return {'status': 'unfixable', 'smiles': None, 'error': 'Initial parsing failed and no fixes were applicable.'}\n",
    "\n",
    "\n",
    "def split_dataset(df, task_name, test_size=0.1, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    将数据集按照8:1:1的比例分割为训练集、验证集和测试集\n",
    "    \n",
    "    Args:\n",
    "        df: 包含数据的DataFrame\n",
    "        task_name: 任务名称\n",
    "        test_size: 测试集比例\n",
    "        val_size: 验证集比例\n",
    "        random_state: 随机种子\n",
    "    \n",
    "    Returns:\n",
    "        train_df, val_df, test_df: 分割后的三个数据集\n",
    "    \"\"\"\n",
    "    if len(df) < 10:  # 如果数据太少，不进行分割\n",
    "        print(f\"  警告: {task_name} 数据量太少 ({len(df)} 样本)，不进行分割\")\n",
    "        return df.copy(), pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # 首先分出测试集\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=None  # 对于回归任务不使用分层抽样\n",
    "    )\n",
    "    \n",
    "    # 再从训练+验证集中分出验证集\n",
    "    if len(train_val_df) < 5:  # 如果剩余数据太少\n",
    "        return train_val_df.copy(), pd.DataFrame(), test_df\n",
    "    \n",
    "    # 计算验证集在剩余数据中的比例\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=val_size_adjusted,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def process_and_split_datasets(input_dir, output_dir, files_to_skip=None, split_ratios=(0.8, 0.1, 0.1), random_state=42, report_unfixable=True):\n",
    "    \"\"\"\n",
    "    加载、修复、诊断、清洗、分割并合并CSV文件。\n",
    "    \n",
    "    Args:\n",
    "        input_dir: 输入目录路径\n",
    "        output_dir: 输出目录路径\n",
    "        files_to_skip: 要跳过的文件列表\n",
    "        split_ratios: 分割比例 (train, val, test)\n",
    "        random_state: 随机种子\n",
    "        report_unfixable: 是否报告无法修复的SMILES\n",
    "    \"\"\"\n",
    "    if files_to_skip is None:\n",
    "        files_to_skip = []\n",
    "    \n",
    "    skip_list_lower = [item.lower().strip() for item in files_to_skip]\n",
    "    \n",
    "    # 存储所有任务的分割数据\n",
    "    all_train_dfs = []\n",
    "    all_val_dfs = []\n",
    "    all_test_dfs = []\n",
    "    \n",
    "    unfixable_report = []\n",
    "    split_summary = []\n",
    "\n",
    "    print(f\"开始处理目录 '{input_dir}' ...\")\n",
    "    print(f\"分割比例: 训练集 {split_ratios[0]:.1%}, 验证集 {split_ratios[1]:.1%}, 测试集 {split_ratios[2]:.1%}\")\n",
    "    \n",
    "    # 创建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.endswith('.csv'): \n",
    "            continue\n",
    "        \n",
    "        task_name_original = filename.split('.')[0]\n",
    "        task_name_normalized = task_name_original.strip().lower()\n",
    "\n",
    "        if task_name_normalized in skip_list_lower: \n",
    "            print(f\"跳过文件: {filename}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- 正在处理: {filename} ---\")\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            if 'SMILES' in df.columns: \n",
    "                df.rename(columns={'SMILES': 'smiles'}, inplace=True)\n",
    "            if 'smiles' not in df.columns or 'Label' not in df.columns: \n",
    "                print(f\"  跳过: 缺少必要的列 (smiles 或 Label)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  原始数据: {len(df)} 样本\")\n",
    "\n",
    "            # 应用诊断和修复函数\n",
    "            diagnostics = df['smiles'].apply(diagnose_and_fix_smiles)\n",
    "            df['smiles_fixed'] = diagnostics.apply(lambda x: x['smiles'])\n",
    "            df['status'] = diagnostics.apply(lambda x: x['status'])\n",
    "            df['error_msg'] = diagnostics.apply(lambda x: x['error'])\n",
    "\n",
    "            # 报告无法修复的SMILES\n",
    "            if report_unfixable:\n",
    "                unfixable_df = df[df['status'] == 'unfixable']\n",
    "                for _, row in unfixable_df.iterrows():\n",
    "                    unfixable_report.append({\n",
    "                        'file': filename,\n",
    "                        'original_smiles': row['smiles'],\n",
    "                        'error': row['error_msg']\n",
    "                    })\n",
    "\n",
    "            # 清洗数据\n",
    "            cleaned_df = df.dropna(subset=['smiles_fixed']).copy()\n",
    "            if cleaned_df.empty: \n",
    "                print(f\"  清洗后无有效数据\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  清洗后数据: {len(cleaned_df)} 样本\")\n",
    "\n",
    "            # 生成InChIKey和重命名\n",
    "            cleaned_df['Inchikey'] = smiles_to_inchikey(cleaned_df['smiles_fixed'])\n",
    "            cleaned_df.rename(columns={'Label': task_name_original}, inplace=True)\n",
    "            final_df = cleaned_df[['smiles_fixed', 'Inchikey', task_name_original]].copy()\n",
    "            final_df.rename(columns={'smiles_fixed': 'smiles'}, inplace=True)\n",
    "            \n",
    "            # 移除InChIKey生成失败的行\n",
    "            final_df = final_df.dropna(subset=['Inchikey'])\n",
    "            \n",
    "            if final_df.empty:\n",
    "                print(f\"  生成InChIKey后无有效数据\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  最终数据: {len(final_df)} 样本\")\n",
    "\n",
    "            # 分割数据集\n",
    "            train_df, val_df, test_df = split_dataset(\n",
    "                final_df, \n",
    "                task_name_original, \n",
    "                test_size=split_ratios[2], \n",
    "                val_size=split_ratios[1], \n",
    "                random_state=random_state\n",
    "            )\n",
    "\n",
    "            print(f\"  分割结果: 训练集 {len(train_df)}, 验证集 {len(val_df)}, 测试集 {len(test_df)}\")\n",
    "\n",
    "            # 记录分割统计\n",
    "            split_summary.append({\n",
    "                'task': task_name_original,\n",
    "                'total': len(final_df),\n",
    "                'train': len(train_df),\n",
    "                'val': len(val_df),\n",
    "                'test': len(test_df),\n",
    "                'train_pct': len(train_df) / len(final_df) * 100 if len(final_df) > 0 else 0,\n",
    "                'val_pct': len(val_df) / len(final_df) * 100 if len(final_df) > 0 else 0,\n",
    "                'test_pct': len(test_df) / len(final_df) * 100 if len(final_df) > 0 else 0\n",
    "            })\n",
    "\n",
    "            # 保存单个任务的分割结果\n",
    "            task_output_dir = os.path.join(output_dir, 'individual_tasks', task_name_original)\n",
    "            os.makedirs(task_output_dir, exist_ok=True)\n",
    "            \n",
    "            if not train_df.empty:\n",
    "                train_df.to_csv(os.path.join(task_output_dir, 'train.csv'), index=False)\n",
    "            if not val_df.empty:\n",
    "                val_df.to_csv(os.path.join(task_output_dir, 'val.csv'), index=False)\n",
    "            if not test_df.empty:\n",
    "                test_df.to_csv(os.path.join(task_output_dir, 'test.csv'), index=False)\n",
    "\n",
    "            # 添加到总的数据集列表\n",
    "            if not train_df.empty:\n",
    "                all_train_dfs.append(train_df)\n",
    "            if not val_df.empty:\n",
    "                all_val_dfs.append(val_df)\n",
    "            if not test_df.empty:\n",
    "                all_test_dfs.append(test_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> 处理文件 {filename} 时发生严重错误: {e}\")\n",
    "\n",
    "    # 合并所有任务的数据集\n",
    "    print(f\"\\n--- 合并所有任务的数据集 ---\")\n",
    "    \n",
    "    def merge_datasets(df_list, set_name):\n",
    "        \"\"\"合并数据集列表\"\"\"\n",
    "        if not df_list:\n",
    "            print(f\"  {set_name}: 没有数据可合并\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"  {set_name}: 合并 {len(df_list)} 个任务的数据\")\n",
    "        merged_df = reduce(\n",
    "            lambda left, right: pd.merge(left, right, on=['smiles', 'Inchikey'], how='outer'), \n",
    "            df_list\n",
    "        )\n",
    "        print(f\"    合并后: {len(merged_df)} 个唯一分子\")\n",
    "        return merged_df\n",
    "\n",
    "    # 合并训练集\n",
    "    merged_train = merge_datasets(all_train_dfs, \"训练集\")\n",
    "    if not merged_train.empty:\n",
    "        train_output_path = os.path.join(output_dir, 'train.csv')\n",
    "        merged_train.drop(['Inchikey'], axis=1, inplace=True, errors='ignore')  # 移除InChIKey列\n",
    "        merged_train.to_csv(train_output_path, index=False)\n",
    "        print(f\"    保存至: {train_output_path}\")\n",
    "\n",
    "    # 合并验证集\n",
    "    merged_val = merge_datasets(all_val_dfs, \"验证集\")\n",
    "    if not merged_val.empty:\n",
    "        val_output_path = os.path.join(output_dir, 'val.csv')\n",
    "        merged_val.drop(['Inchikey'], axis=1, inplace=True, errors='ignore')  # 移除InChIKey列\n",
    "        merged_val.to_csv(val_output_path, index=False)\n",
    "        print(f\"    保存至: {val_output_path}\")\n",
    "\n",
    "    # 合并测试集\n",
    "    merged_test = merge_datasets(all_test_dfs, \"测试集\")\n",
    "    if not merged_test.empty:\n",
    "        test_output_path = os.path.join(output_dir, 'test.csv')\n",
    "        merged_test.drop(['Inchikey'], axis=1, inplace=True, errors='ignore')  # 移除InChIKey列\n",
    "        merged_test.to_csv(test_output_path, index=False)\n",
    "        print(f\"    保存至: {test_output_path}\")\n",
    "\n",
    "    # 保存分割统计报告\n",
    "    if split_summary:\n",
    "        summary_df = pd.DataFrame(split_summary)\n",
    "        summary_path = os.path.join(output_dir, 'split_summary.csv')\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        \n",
    "        print(f\"\\n--- 分割统计报告 ---\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "        print(f\"详细统计已保存至: {summary_path}\")\n",
    "\n",
    "    # 合并所有数据（不分割）用于对比\n",
    "    print(f\"\\n--- 生成完整合并数据集 ---\")\n",
    "    if all_train_dfs or all_val_dfs or all_test_dfs:\n",
    "        # 收集所有任务的完整数据\n",
    "        all_complete_dfs = []\n",
    "        \n",
    "        for filename in os.listdir(input_dir):\n",
    "            if not filename.endswith('.csv'): \n",
    "                continue\n",
    "            \n",
    "            task_name_original = filename.split('.')[0]\n",
    "            task_name_normalized = task_name_original.strip().lower()\n",
    "\n",
    "            if task_name_normalized in skip_list_lower: \n",
    "                continue\n",
    "            \n",
    "            # 重新读取并处理（这次不分割）\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                if 'SMILES' in df.columns: \n",
    "                    df.rename(columns={'SMILES': 'smiles'}, inplace=True)\n",
    "                if 'smiles' not in df.columns or 'Label' not in df.columns: \n",
    "                    continue\n",
    "\n",
    "                # 诊断和修复\n",
    "                diagnostics = df['smiles'].apply(diagnose_and_fix_smiles)\n",
    "                df['smiles_fixed'] = diagnostics.apply(lambda x: x['smiles'])\n",
    "                cleaned_df = df.dropna(subset=['smiles_fixed']).copy()\n",
    "                \n",
    "                if cleaned_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # 处理最终数据\n",
    "                cleaned_df['Inchikey'] = smiles_to_inchikey(cleaned_df['smiles_fixed'])\n",
    "                cleaned_df.rename(columns={'Label': task_name_original}, inplace=True)\n",
    "                final_df = cleaned_df[['smiles_fixed', 'Inchikey', task_name_original]].copy()\n",
    "                final_df.rename(columns={'smiles_fixed': 'smiles'}, inplace=True)\n",
    "                final_df = final_df.dropna(subset=['Inchikey'])\n",
    "                \n",
    "                if not final_df.empty:\n",
    "                    all_complete_dfs.append(final_df)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        if all_complete_dfs:\n",
    "            complete_merged = reduce(\n",
    "                lambda left, right: pd.merge(left, right, on=['smiles', 'Inchikey'], how='outer'), \n",
    "                all_complete_dfs\n",
    "            )\n",
    "            complete_path = os.path.join(output_dir, 'complete_dataset.csv')\n",
    "            complete_merged.to_csv(complete_path, index=False)\n",
    "            print(f\"完整数据集: {len(complete_merged)} 个分子，保存至: {complete_path}\")\n",
    "\n",
    "    # 打印无法修复的SMILES报告\n",
    "    if unfixable_report:\n",
    "        print(\"\\n--- 无法修复的SMILES报告 ---\")\n",
    "        report_df = pd.DataFrame(unfixable_report)\n",
    "        unfixable_path = os.path.join(output_dir, 'unfixable_smiles_report.csv')\n",
    "        report_df.to_csv(unfixable_path, index=False)\n",
    "        print(f\"发现 {len(unfixable_report)} 个无法修复的SMILES\")\n",
    "        print(f\"详细报告已保存至: {unfixable_path}\")\n",
    "        \n",
    "        if len(unfixable_report) <= 10:\n",
    "            print(report_df.to_string(index=False))\n",
    "\n",
    "    print(f\"\\n=== 处理完成 ===\")\n",
    "    print(f\"所有结果保存在目录: {output_dir}\")\n",
    "    print(f\"  - train.csv: 合并的训练集\")\n",
    "    print(f\"  - val.csv: 合并的验证集\") \n",
    "    print(f\"  - test.csv: 合并的测试集\")\n",
    "    print(f\"  - complete_dataset.csv: 完整合并数据集\")\n",
    "    print(f\"  - individual_tasks/: 各任务的单独分割结果\")\n",
    "    print(f\"  - split_summary.csv: 分割统计报告\")\n",
    "\n",
    "\n",
    "# --- 执行 ---\n",
    "if __name__ == '__main__':\n",
    "    # --- 测试新的诊断函数 ---\n",
    "    print(\"--- 测试诊断功能 ---\")\n",
    "    test_smiles = 'BrN([C-2]C)([C-2]C)([C-2]C)[C-2]C'\n",
    "    result = diagnose_and_fix_smiles(test_smiles)\n",
    "    print(f\"原始SMILES: {test_smiles}\")\n",
    "    print(f\"诊断结果: {result}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # --- 运行数据分割和合并流程 ---\n",
    "    INPUT_REG_DIR = 'ademt_data/wash_reg'\n",
    "    OUTPUT_DIR = 'ademt_data/reg_3'\n",
    "    SKIP_FILES = []  # 可以指定要跳过的文件，例如 ['logD', 'logP']\n",
    "    \n",
    "    # 设置分割参数\n",
    "    SPLIT_RATIOS = (0.8, 0.1, 0.1)  # 训练集, 验证集, 测试集\n",
    "    RANDOM_STATE = 42  # 随机种子，确保结果可重现\n",
    "\n",
    "    process_and_split_datasets(\n",
    "        input_dir=INPUT_REG_DIR, \n",
    "        output_dir=OUTPUT_DIR, \n",
    "        files_to_skip=SKIP_FILES,\n",
    "        split_ratios=SPLIT_RATIOS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        report_unfixable=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4669800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REG1 数据集分析 ===\n",
      "总样本数: 23,198\n",
      "\n",
      "各任务数据统计:\n",
      "  cl-plasma   :   852 样本 ( 96.3% 缺失)\n",
      "  logvdss     : 2,448 样本 ( 89.4% 缺失)\n",
      "  t12         : 3,441 样本 ( 85.2% 缺失)\n",
      "  mdck        : 1,144 样本 ( 95.1% 缺失)\n",
      "  caco2       : 6,512 样本 ( 71.9% 缺失)\n",
      "  ppb         : 4,737 样本 ( 79.6% 缺失)\n",
      "  cl-int      : 7,893 样本 ( 66.0% 缺失)\n",
      "\n",
      "建议权重分配 (基于样本稀疏程度):\n",
      "  cl-plasma   : 权重 4.9\n",
      "  logvdss     : 权重 4.6\n",
      "  t12         : 权重 4.4\n",
      "  mdck        : 权重 4.8\n",
      "  caco2       : 权重 3.9\n",
      "  ppb         : 权重 4.2\n",
      "  cl-int      : 权重 3.6\n"
     ]
    }
   ],
   "source": [
    "# 先分析reg1数据集的稀疏性\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/mnt/newdisk/fuli/ADMET/ademt_data/reg1.csv')\n",
    "tasks = ['cl-plasma', 'logvdss', 't12', 'mdck', 'caco2', 'ppb', 'cl-int']\n",
    "\n",
    "print('=== REG1 数据集分析 ===')\n",
    "print(f'总样本数: {len(df):,}')\n",
    "print()\n",
    "\n",
    "print('各任务数据统计:')\n",
    "task_counts = {}\n",
    "for task in tasks:\n",
    "    valid_count = df[task].notna().sum()\n",
    "    missing_rate = (len(df) - valid_count) / len(df) * 100\n",
    "    task_counts[task] = valid_count\n",
    "    print(f'  {task:12}: {valid_count:5,} 样本 ({missing_rate:5.1f}% 缺失)')\n",
    "\n",
    "print()\n",
    "print('建议权重分配 (基于样本稀疏程度):')\n",
    "total_samples = sum(task_counts.values())\n",
    "for task in tasks:\n",
    "    # 稀疏度越高，权重越大\n",
    "    sparsity = 1.0 - (task_counts[task] / len(df))\n",
    "    suggested_weight = 1.0 + sparsity * 4  # 基础权重1.0，最大额外权重4.0\n",
    "    print(f'  {task:12}: 权重 {suggested_weight:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15d6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
